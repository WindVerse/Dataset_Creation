{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a863fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f93e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Core Spatial Fusion Module (Single Timestep)\n",
    "# ================================\n",
    "class FlagWindNet(nn.Module):\n",
    "    def __init__(self, flag_h= 32, flag_w = 32, num_wind_points=8, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Grid dimensions (Assumed 32x32 = 1024)\n",
    "        self.grid_h = flag_h\n",
    "        self.grid_w = flag_w\n",
    "\n",
    "        # 1. CNN Flag Encoder (Takes 32x32x3 image structure)\n",
    "        # Input: (Batch, 3, 32, 32) -> Output: Flattened Vector\n",
    "        self.flag_encoder = nn.Sequential(\n",
    "            # Conv Block 1: 3 -> 16\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # Output: 16 x 16 x 16\n",
    "\n",
    "            # Conv Block 2: 16 -> 32\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # Output: 32 x 8 x 8\n",
    "\n",
    "            # Conv Block 3: 32 -> 64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # Output: 64 x 4 x 4\n",
    "\n",
    "            # Flatten final feature map\n",
    "            nn.Flatten(),\n",
    "            # Project to latent dim (64 * 4 * 4 = 1024)\n",
    "            nn.Linear(64 * 4 * 4, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # 2. Wind Encoder (MLP)\n",
    "        self.wind_flat_dim = num_wind_points * 3\n",
    "        self.wind_encoder = nn.Sequential(\n",
    "            nn.Linear(self.wind_flat_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        )\n",
    "\n",
    "        # LSTM input size = Flag Latent + Wind Latent\n",
    "        self.lstm_input_dim = hidden_dim + (hidden_dim // 4)\n",
    "\n",
    "        # 3. Temporal Processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ==========================================\n",
    "        # 4. DECODER (4x4 -> 32x32)\n",
    "        # ==========================================\n",
    "\n",
    "        # Step A: Project LSTM Hidden State back to Spatial Feature Map Size\n",
    "        # We need to get back to (64 channels, 4 height, 4 width)\n",
    "        self.decoder_projection = nn.Linear(hidden_dim, 64 * 4 * 4)\n",
    "\n",
    "        # Step B: Transpose Convolutions to Upsample\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            # Unflatten happen manually in forward()\n",
    "\n",
    "            # Block 1: 4x4 -> 8x8\n",
    "            # Input: (B, 64, 4, 4)\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Block 2: 8x8 -> 16x16\n",
    "            # Input: (B, 32, 8, 8)\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Block 3: 16x16 -> 32x32\n",
    "            # Input: (B, 16, 16, 16)\n",
    "            # Output channels = 3 (dx, dy, dz)\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=3,\n",
    "                               kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            # No final activation (Regression output can be pos or neg)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_flag_seq, x_wind_seq):\n",
    "        # x_flag_seq: (B, L, 1024, 3)\n",
    "        # x_wind_seq: (B, L, 8, 3)\n",
    "\n",
    "        B, L, N, D = x_flag_seq.shape\n",
    "\n",
    "        # --- 1. PREPARE INPUTS ---\n",
    "        # Reshape to (B*L, 3, 32, 32)\n",
    "        x_flag_img = x_flag_seq.view(B * L, self.grid_h, self.grid_w, 3).permute(0, 3, 1, 2)\n",
    "        x_wind_flat = x_wind_seq.view(B * L, -1)\n",
    "\n",
    "        # --- 2. ENCODE ---\n",
    "        # Flag: CNN -> Flatten -> Linear\n",
    "        # The encoder now outputs the latent vector directly\n",
    "        flag_latent = self.flag_encoder(x_flag_img)    # (B*L, hidden_dim)\n",
    "\n",
    "        # Wind: MLP\n",
    "        wind_latent = self.wind_encoder(x_wind_flat)   # (B*L, hidden_dim//4)\n",
    "\n",
    "        # --- 3. LSTM ---\n",
    "        # Combine Latents\n",
    "        combined = torch.cat([flag_latent, wind_latent], dim=1) # (B*L, H + H/4)\n",
    "\n",
    "        # Reshape for LSTM: (B, L, Input_Size)\n",
    "        lstm_in = combined.view(B, L, -1)\n",
    "\n",
    "        # Run LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_in) # Output: (B, L, hidden_dim)\n",
    "\n",
    "        # --- 4. DECODE ---\n",
    "        # Flatten time dim again: (B*L, hidden_dim)\n",
    "        lstm_out_flat = lstm_out.reshape(B * L, -1)\n",
    "\n",
    "        # Project back to feature map size: (B*L, 64*4*4)\n",
    "        decoder_input = self.decoder_projection(lstm_out_flat)\n",
    "\n",
    "        # Unflatten to 4x4 spatial map: (B*L, 64, 4, 4)\n",
    "        decoder_input_map = decoder_input.view(B * L, 64, 4, 4)\n",
    "\n",
    "        # Run Transpose Convs: Output (B*L, 3, 32, 32)\n",
    "        spatial_output = self.decoder_cnn(decoder_input_map)\n",
    "\n",
    "        # --- 5. FORMAT OUTPUT ---\n",
    "        # (B*L, 3, 32, 32) -> (B*L, 32, 32, 3) -> (B, L, 1024, 3)\n",
    "        output_seq = spatial_output.permute(0, 2, 3, 1).reshape(B, L, N, D)\n",
    "\n",
    "        return output_seq\n",
    "\n",
    "# # ================================\n",
    "# # 2. Sequential Model\n",
    "# # ================================\n",
    "# class SequentialFlagWindNet(nn.Module):\n",
    "#     def __init__(self, num_flag_points=1024,\n",
    "#                  num_wind_points=8, hidden_dim=128):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Dimensions\n",
    "#         self.flag_flat_dim = num_flag_points * 3\n",
    "#         self.wind_flat_dim = num_wind_points * 3\n",
    "\n",
    "#         # 1. Input Compression\n",
    "#         # FIXED: Added commas between layers\n",
    "#         self.flag_encoder = nn.Sequential(\n",
    "#             nn.Linear(self.flag_flat_dim, hidden_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "#         )\n",
    "\n",
    "#         # FIXED: Input to 2nd layer matches output of 1st layer (hidden_dim // 4)\n",
    "#         self.wind_encoder = nn.Sequential(\n",
    "#             nn.Linear(self.wind_flat_dim, hidden_dim // 4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim // 4, hidden_dim // 8)\n",
    "#         )\n",
    "\n",
    "#         # LSTM input size = (H/4) + (H/8) = 3H/8\n",
    "#         self.lstm_input_dim = (hidden_dim // 4) + (hidden_dim // 8)\n",
    "\n",
    "#         # 2. Temporal Processing\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=self.lstm_input_dim,\n",
    "#             hidden_size=hidden_dim,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "\n",
    "#         # 3. Decoder\n",
    "#         # FIXED: The decoder must map the LSTM Output (hidden_dim)\n",
    "#         # back to the full flag size (1024 * 3).\n",
    "#         # We cannot reuse 'FlagWindSpatialFusionNet' here because that net\n",
    "#         # expects raw 3D coordinates, but we have a compressed LSTM vector.\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim * 2, self.flag_flat_dim) # Output: 3072\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_flag_seq, x_wind_seq):\n",
    "#         # x_flag_seq: (B, L, 1024, 3)\n",
    "#         # x_wind_seq: (B, L, 8, 3)\n",
    "#         B, L, N, D = x_flag_seq.shape\n",
    "\n",
    "#         # Flatten\n",
    "#         flag_flat = x_flag_seq.reshape(B*L, -1)\n",
    "#         wind_flat = x_wind_seq.reshape(B*L, -1)\n",
    "\n",
    "#         # Encode\n",
    "#         flag_enc = self.flag_encoder(flag_flat) # (B*L, H/4)\n",
    "#         wind_enc = self.wind_encoder(wind_flat) # (B*L, H/8)\n",
    "\n",
    "#         # Combine\n",
    "#         combined = torch.cat([flag_enc, wind_enc], dim=1) # (B*L, 3H/8)\n",
    "\n",
    "#         # Reshape for LSTM\n",
    "#         lstm_in = combined.reshape(B, L, -1)\n",
    "\n",
    "#         # LSTM Pass\n",
    "#         lstm_out, _ = self.lstm(lstm_in) # (B, L, H)\n",
    "\n",
    "#         # Decode from LSTM output\n",
    "#         lstm_out_flat = lstm_out.reshape(B*L, -1)\n",
    "\n",
    "#         # FIXED: Decoder uses the LSTM output\n",
    "#         disp_flat = self.decoder(lstm_out_flat)\n",
    "\n",
    "#         # Reshape back to sequence\n",
    "#         return disp_flat.reshape(B, L, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "782eab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================\n",
    "# # 2. Model\n",
    "# # ================================\n",
    "# class FlagWindNet(nn.Module):\n",
    "#     def __init__(self, hidden_size=512):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Encode per-flag-point (3 → hidden_size)\n",
    "#         self.flag_encoder = nn.Sequential(\n",
    "#             nn.Linear(3, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Encode per-wind-point (3 → hidden_size)\n",
    "#         self.wind_encoder = nn.Sequential(\n",
    "#             nn.Linear(3, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Combine wind into a global embedding (mean pooling)\n",
    "#         self.wind_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "#         # Final decoder: per-flag-point + wind → next (x,y,z)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(hidden_size * 2, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, 3)   # predict next xyz\n",
    "#         )\n",
    "\n",
    "#     def forward(self, flag, wind):\n",
    "#         \"\"\"\n",
    "#         flag: (B, 1024, 3)\n",
    "#         wind: (B, 8, 3)\n",
    "#         \"\"\"\n",
    "#         B, N, _ = flag.shape\n",
    "\n",
    "#         # Encode flag points → (B, N, hidden)\n",
    "#         flag_feat = self.flag_encoder(flag)  # applies per point\n",
    "\n",
    "#         # Encode wind points → (B, 8, hidden)\n",
    "#         wind_feat = self.wind_encoder(wind)\n",
    "\n",
    "#         # Pool wind to global embedding → (B, hidden)\n",
    "#         wind_global = wind_feat.mean(dim=1)\n",
    "\n",
    "#         # Broadcast wind_global to match each flag point → (B, N, hidden)\n",
    "#         wind_broadcast = wind_global.unsqueeze(1).expand(-1, N, -1)\n",
    "\n",
    "#         # Concatenate flag and wind features\n",
    "#         fused = torch.cat([flag_feat, wind_broadcast], dim=-1)  # (B, N, 2*hidden)\n",
    "\n",
    "#         # Decode per-flag-point → (B, N, 3)\n",
    "#         out = self.decoder(fused)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Load the full model object ----\n",
    "# with torch.serialization.safe_globals([FlagWindNet]):\n",
    "#     model = torch.load(\"flagwind_model.pth\", map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d922ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/flagwind_model_weights.pth...\n",
      "Model loaded successfully.\n",
      "✅ Successfully exported model to flag_wind_net.onnx\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2. LOAD AND EXPORT\n",
    "# ================================\n",
    "\n",
    "# --- Configuration ---\n",
    "# !! SET THIS: Path to your trained model\n",
    "MODEL_PATH = \"./model/flagwind_model_weights.pth\" \n",
    "EXPORT_NAME = \"./model/flagwind_model.onnx\" # Output file name\n",
    "NUM_VERTICES = 1024 # Must match your flag's vertex count\n",
    "\n",
    "# --- Load Model ---\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"!! ERROR: Model file not found at: {MODEL_PATH}\")\n",
    "    print(\"Please make sure the file is in the same directory or provide the full path.\")\n",
    "else:\n",
    "    # HERE IS THE FIX: Instantiate with default parameters\n",
    "    model = FlagWindNet() \n",
    "    \n",
    "    # Load the weights\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
    "    \n",
    "    # IMPORTANT: Set model to evaluation mode\n",
    "    model.eval() \n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # # --- Create Dummy Inputs ---\n",
    "    # # We need to provide example inputs so ONNX can trace the model's operations.\n",
    "    # # The batch size (B) should be 1.\n",
    "    # dummy_flag_input = torch.randn(1, NUM_VERTICES, 3)\n",
    "    # dummy_wind_input = torch.randn(1, 8, 3) # Based on your model's forward pass\n",
    "\n",
    "    # # --- Define Input/Output Names ---\n",
    "    # # These names will be used in your C# script to feed data to the model.\n",
    "    # input_names = [\"flag_input\", \"wind_input\"]\n",
    "    # output_names = [\"displacement_output\"]\n",
    "\n",
    "    # # --- Export ---\n",
    "    # print(f\"Exporting model to {EXPORT_NAME}...\")\n",
    "    # torch.onnx.export(\n",
    "    #     model,\n",
    "    #     (dummy_flag_input, dummy_wind_input), # Dummy inputs tuple\n",
    "    #     EXPORT_NAME,\n",
    "    #     input_names=input_names,\n",
    "    #     output_names=output_names,\n",
    "    #     opset_version=12, # A good, stable version\n",
    "    #     verbose=False\n",
    "    # )\n",
    "    # print(\"=\"*30)\n",
    "    # print(f\"✅ Successfully exported model to {EXPORT_NAME}\")\n",
    "    # print(f\"  Input 1 ({input_names[0]}): shape (1, {NUM_VERTICES}, 3)\")\n",
    "    # print(f\"  Input 2 ({input_names[1]}): shape (1, 8, 3)\")\n",
    "    # print(f\"  Output ({output_names[0]}): shape (1, {NUM_VERTICES}, 3)\")\n",
    "    # print(\"=\"*30)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --- 1. Define Correct 4D Dummy Inputs ---\n",
    "    # Shape: (Batch=1, Seq_Len=5, Points=1024, Channels=3)\n",
    "    dummy_flag_input = torch.randn(1, 5, 1024, 3) \n",
    "\n",
    "    # Shape: (Batch=1, Seq_Len=5, Wind_Points=8, Channels=3)\n",
    "    dummy_wind_input = torch.randn(1, 5, 8, 3)\n",
    "\n",
    "    # --- 2. Run the Export ---\n",
    "    EXPORT_NAME = \"flag_wind_net.onnx\"\n",
    "    input_names = [\"flag_input\", \"wind_input\"]\n",
    "    output_names = [\"displacement_output\"]\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_flag_input, dummy_wind_input),\n",
    "        EXPORT_NAME,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        opset_version=12,\n",
    "        # IMPORTANT: Allow Batch (0) and Sequence (1) dimensions to change size\n",
    "        dynamic_axes={\n",
    "            \"flag_input\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"wind_input\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"displacement_output\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "        },\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Successfully exported model to {EXPORT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5132e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
